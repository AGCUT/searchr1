#!/usr/bin/env python3
"""
BadCase é”™è¯¯ç±»å‹åˆ†æå·¥å…·

åˆ†ç±»æ ‡å‡†ï¼š
1. æ²¡æœ‰ç»™å‡ºç­”æ¡ˆï¼šæ¨¡å‹æ²¡æœ‰ç”Ÿæˆ <answer> æ ‡ç­¾
2. æ£€ç´¢é”™è¯¯ï¼šæ£€ç´¢æ–‡æ¡£ä¸­ä¸åŒ…å«æ­£ç¡®ç­”æ¡ˆ
3. æ¨ç†é”™è¯¯ï¼šæ£€ç´¢åˆ°äº†æ­£ç¡®ç­”æ¡ˆï¼Œä½†æ¨¡å‹æå–/ç†è§£é”™è¯¯
"""

import json
import re
import argparse
from typing import List, Dict, Tuple
from dataclasses import dataclass
from collections import defaultdict


@dataclass
class BadCaseAnalysis:
    """BadCase åˆ†æç»“æœ"""
    question: str
    golden_answer: str
    extracted_answer: str
    data_source: str
    num_searches: int
    error_type: str  # "no_answer", "retrieval_error", "reasoning_error"
    explanation: str
    full_trajectory: str


def normalize_answer(text: str) -> str:
    """æ ‡å‡†åŒ–ç­”æ¡ˆç”¨äºåŒ¹é…"""
    if not text:
        return ""
    text = text.lower().strip()
    # ç§»é™¤æ ‡ç‚¹å’Œå¤šä½™ç©ºæ ¼
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text


def check_answer_in_text(golden_answers: List[str], text: str) -> Tuple[bool, str]:
    """
    æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨æ–‡æœ¬ä¸­

    Returns:
        (found, matched_answer)
    """
    text_normalized = normalize_answer(text)

    for answer in golden_answers:
        answer_normalized = normalize_answer(answer)

        # å®Œå…¨åŒ¹é…
        if answer_normalized in text_normalized:
            return True, answer

        # åˆ†è¯ååŒ¹é…ï¼ˆå¤„ç†å¤šè¯ç­”æ¡ˆï¼‰
        answer_words = answer_normalized.split()
        if len(answer_words) > 1:
            # æ£€æŸ¥æ‰€æœ‰å…³é”®è¯æ˜¯å¦éƒ½å‡ºç°
            if all(word in text_normalized for word in answer_words):
                return True, answer

    return False, ""


def parse_golden_answer(golden_answer_str: str) -> List[str]:
    """è§£æ golden_answer å­—ç¬¦ä¸²"""
    # å°è¯•è§£æ {'target': array([...])} æ ¼å¼
    if "array([" in golden_answer_str:
        match = re.search(r"array\(\[(.*?)\]", golden_answer_str, re.DOTALL)
        if match:
            content = match.group(1)
            # æå–æ‰€æœ‰å¼•å·å†…çš„å†…å®¹
            answers = re.findall(r"'([^']*)'", content)
            if not answers:
                answers = re.findall(r'"([^"]*)"', content)
            return [a.strip() for a in answers if a.strip()]

    # å¦‚æœæ˜¯ç®€å•å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
    return [golden_answer_str.strip()]


def extract_retrieved_docs(trajectory: str) -> List[str]:
    """æå–æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹"""
    docs = []
    pattern = r'<information>(.*?)</information>'
    matches = re.findall(pattern, trajectory, re.DOTALL)

    for match in matches:
        docs.append(match.strip())

    return docs


def check_query_format_errors(trajectory: str) -> Tuple[bool, List[str]]:
    """
    æ£€æŸ¥æ˜¯å¦å­˜åœ¨æŸ¥è¯¢æ ¼å¼é”™è¯¯

    Returns:
        (has_error, error_examples)
    """
    errors = []

    # æ£€æŸ¥é”™è¯¯æ ¼å¼ 1: <search query="..."> </search>
    pattern1 = r'<search\s+query="([^"]*)">\s*</search>'
    matches1 = re.findall(pattern1, trajectory, re.IGNORECASE)
    if matches1:
        for match in matches1[:3]:  # æœ€å¤šå±•ç¤º 3 ä¸ª
            errors.append(f'æ ¼å¼é”™è¯¯: <search query="{match}"> </search>')

    # æ£€æŸ¥é”™è¯¯æ ¼å¼ 2: <search></search> (ç©ºæŸ¥è¯¢)
    pattern2 = r'<search>\s*</search>'
    if re.search(pattern2, trajectory):
        errors.append('æ ¼å¼é”™è¯¯: <search></search> (ç©ºæŸ¥è¯¢)')

    # æ£€æŸ¥é”™è¯¯æ ¼å¼ 3: <search>...(æ²¡æœ‰é—­åˆæ ‡ç­¾)
    pattern3 = r'<search>([^<]*?)(?:<think>|<answer>|$)'
    matches3 = re.findall(pattern3, trajectory, re.DOTALL)
    for match in matches3:
        if '</search>' not in match and len(match.strip()) > 0:
            query_preview = match.strip()[:50]
            errors.append(f'æ ¼å¼é”™è¯¯: <search>{query_preview}... (æœªé—­åˆ)')
            break  # åªå±•ç¤ºç¬¬ä¸€ä¸ª

    # æ£€æŸ¥é”™è¯¯æ ¼å¼ 4: æŸ¥è¯¢å¤ªé•¿æˆ–åŒ…å«ç‰¹æ®Šç»“æ„ï¼ˆå¯èƒ½æ˜¯è¯¯æŠŠæ•´æ®µè¯æ”¾è¿›å»ï¼‰
    pattern4 = r'<search>(.*?)</search>'
    matches4 = re.findall(pattern4, trajectory, re.DOTALL)
    for match in matches4:
        if len(match) > 200:  # æŸ¥è¯¢è¶…è¿‡ 200 å­—ç¬¦ï¼Œå¼‚å¸¸
            errors.append(f'æŸ¥è¯¢è¿‡é•¿: {len(match)} å­—ç¬¦ (æ­£å¸¸åº” < 50)')
            break

    return len(errors) > 0, errors


def classify_badcase(result: Dict) -> BadCaseAnalysis:
    """
    åˆ†ç±» BadCase çš„é”™è¯¯ç±»å‹

    åˆ†ç±»é€»è¾‘ï¼š
    0. é¦–å…ˆæ£€æŸ¥æŸ¥è¯¢æ ¼å¼é”™è¯¯ â†’ "query_format_error"
    1. extracted_answer ä¸ºç©ºï¼š
       1.1 æ£€ç´¢åˆ°äº†æ­£ç¡®ç­”æ¡ˆ â†’ "no_answer_with_correct_docs"ï¼ˆæœ€å…³é”®ï¼ï¼‰
       1.2 æ²¡æ£€ç´¢åˆ°æ­£ç¡®ç­”æ¡ˆ â†’ "no_answer_retrieval_error"
    2. extracted_answer ä¸ä¸ºç©ºï¼š
       2.1 æ£€ç´¢æ–‡æ¡£ä¸­ä¸åŒ…å«æ­£ç¡®ç­”æ¡ˆ â†’ "retrieval_error"ï¼ˆæ£€ç´¢é”™è¯¯ï¼‰
       2.2 æ£€ç´¢æ–‡æ¡£ä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆ â†’ "reasoning_error"ï¼ˆæ¨ç†é”™è¯¯ï¼‰
    """
    question = result.get('question', '')
    golden_answer_str = result.get('golden_answer', '')
    extracted_answer = result.get('extracted_answer', '')
    data_source = result.get('data_source', 'unknown')
    num_searches = result.get('num_searches', 0)
    full_trajectory = result.get('full_trajectory', '') or result.get('model_answer', '')

    # è§£ææ­£ç¡®ç­”æ¡ˆ
    golden_answers = parse_golden_answer(golden_answer_str)

    # æå–æ£€ç´¢æ–‡æ¡£
    retrieved_docs = extract_retrieved_docs(full_trajectory)
    all_retrieved_text = "\n".join(retrieved_docs)

    # æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨æ£€ç´¢æ–‡æ¡£ä¸­
    found_in_docs, matched_answer = check_answer_in_text(golden_answers, all_retrieved_text)

    # ç±»å‹ 0ï¼šæŸ¥è¯¢æ ¼å¼é”™è¯¯ï¼ˆä¼˜å…ˆæ£€æŸ¥ï¼‰
    has_query_error, query_errors = check_query_format_errors(full_trajectory)
    if has_query_error:
        error_detail = "; ".join(query_errors)
        return BadCaseAnalysis(
            question=question,
            golden_answer=golden_answer_str,
            extracted_answer=extracted_answer,
            data_source=data_source,
            num_searches=num_searches,
            error_type="query_format_error",
            explanation=f"æŸ¥è¯¢æ ¼å¼é”™è¯¯å¯¼è‡´æœç´¢å¤±è´¥: {error_detail}",
            full_trajectory=full_trajectory
        )

    # ç±»å‹ 1ï¼šæ²¡æœ‰ç»™å‡ºç­”æ¡ˆï¼ˆç»†åˆ†ä¸ºä¸¤ç§æƒ…å†µï¼‰
    if not extracted_answer or extracted_answer.strip() == "":
        if found_in_docs:
            # 1.1 æ£€ç´¢åˆ°äº†æ­£ç¡®ç­”æ¡ˆï¼Œä½†æ¨¡å‹æ²¡ç”Ÿæˆ <answer>
            return BadCaseAnalysis(
                question=question,
                golden_answer=golden_answer_str,
                extracted_answer=extracted_answer,
                data_source=data_source,
                num_searches=num_searches,
                error_type="no_answer_with_correct_docs",
                explanation=f"æ¨¡å‹è¿›è¡Œäº† {num_searches} æ¬¡æœç´¢ï¼Œæ£€ç´¢åˆ°äº†åŒ…å«æ­£ç¡®ç­”æ¡ˆ '{matched_answer}' çš„æ–‡æ¡£ï¼Œä½†æ²¡æœ‰ç”Ÿæˆ <answer> æ ‡ç­¾",
                full_trajectory=full_trajectory
            )
        else:
            # 1.2 æ£€ç´¢æ²¡æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œä¹Ÿæ²¡ç”Ÿæˆ <answer>
            return BadCaseAnalysis(
                question=question,
                golden_answer=golden_answer_str,
                extracted_answer=extracted_answer,
                data_source=data_source,
                num_searches=num_searches,
                error_type="no_answer_retrieval_error",
                explanation=f"æ¨¡å‹è¿›è¡Œäº† {num_searches} æ¬¡æœç´¢ï¼Œæ£€ç´¢æ–‡æ¡£ä¸­ä¸åŒ…å«æ­£ç¡®ç­”æ¡ˆ '{golden_answers[0]}'ï¼Œä¸”æ²¡æœ‰ç”Ÿæˆ <answer> æ ‡ç­¾",
                full_trajectory=full_trajectory
            )

    # ç±»å‹ 2ï¼šç»™å‡ºäº†ç­”æ¡ˆï¼Œä½†æ£€ç´¢é”™è¯¯
    if not found_in_docs:
        return BadCaseAnalysis(
            question=question,
            golden_answer=golden_answer_str,
            extracted_answer=extracted_answer,
            data_source=data_source,
            num_searches=num_searches,
            error_type="retrieval_error",
            explanation=f"æ¨¡å‹è¿›è¡Œäº† {num_searches} æ¬¡æœç´¢ï¼Œä½†æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­ä¸åŒ…å«æ­£ç¡®ç­”æ¡ˆ '{golden_answers[0]}'ï¼Œæ¨¡å‹å›ç­”äº† '{extracted_answer}'",
            full_trajectory=full_trajectory
        )

    # ç±»å‹ 3ï¼šç»™å‡ºäº†ç­”æ¡ˆï¼Œæ£€ç´¢ä¹Ÿæ­£ç¡®ï¼Œä½†æ¨ç†é”™è¯¯
    return BadCaseAnalysis(
        question=question,
        golden_answer=golden_answer_str,
        extracted_answer=extracted_answer,
        data_source=data_source,
        num_searches=num_searches,
        error_type="reasoning_error",
        explanation=f"æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­åŒ…å«æ­£ç¡®ç­”æ¡ˆ '{matched_answer}'ï¼Œä½†æ¨¡å‹æå–æˆäº† '{extracted_answer}'",
        full_trajectory=full_trajectory
    )


def analyze_badcases(json_path: str, output_path: str = None, sample_size: int = 10):
    """åˆ†æ BadCase å¹¶ç”ŸæˆæŠ¥å‘Š"""

    # è¯»å– JSON
    print(f"è¯»å–è¯„ä¼°ç»“æœ: {json_path}")
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    results = data.get('results', [])

    # ç­›é€‰é”™è¯¯æ ·æœ¬
    badcases = [r for r in results if not r.get('is_correct', False)]

    print(f"æ€»æ ·æœ¬æ•°: {len(results)}")
    print(f"é”™è¯¯æ ·æœ¬æ•°: {len(badcases)}")
    print(f"\nå¼€å§‹åˆ†æ BadCase ç±»å‹...\n")

    # åˆ†ç±»ç»Ÿè®¡
    error_types = defaultdict(list)
    source_error_types = defaultdict(lambda: defaultdict(int))

    for result in badcases:
        analysis = classify_badcase(result)
        error_types[analysis.error_type].append(analysis)
        source_error_types[analysis.data_source][analysis.error_type] += 1

    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("=" * 100)
    print("BadCase é”™è¯¯ç±»å‹ç»Ÿè®¡")
    print("=" * 100)
    print()

    total = len(badcases)

    print(f"{'é”™è¯¯ç±»å‹':<30} {'æ•°é‡':>10} {'å æ¯”':>10} {'è¯´æ˜':<40}")
    print("-" * 100)

    type_info = {
        "query_format_error": "ğŸ”´ã€ä¸¥é‡ã€‘æŸ¥è¯¢æ ¼å¼é”™è¯¯ï¼ˆå¯¼è‡´æœç´¢å¤±è´¥ï¼‰",
        "no_answer_with_correct_docs": "ğŸŸ ã€é‡ç‚¹ã€‘æ£€ç´¢åˆ°æ­£ç¡®ç­”æ¡ˆï¼Œä½†æ²¡ç”Ÿæˆ <answer>",
        "no_answer_retrieval_error": "ğŸŸ¡ æ£€ç´¢é”™è¯¯ + æ²¡ç”Ÿæˆ <answer>",
        "retrieval_error": "ğŸŸ¢ æ£€ç´¢é”™è¯¯ï¼ˆç»™å‡ºäº†é”™è¯¯ç­”æ¡ˆï¼‰",
        "reasoning_error": "ğŸ”µ æ¨ç†é”™è¯¯ï¼ˆæ£€ç´¢åˆ°äº†ä½†ç†è§£/æå–é”™è¯¯ï¼‰"
    }

    for error_type in ["query_format_error", "no_answer_with_correct_docs", "no_answer_retrieval_error", "retrieval_error", "reasoning_error"]:
        count = len(error_types[error_type])
        percentage = count / total * 100
        info = type_info.get(error_type, "")
        print(f"{error_type:<30} {count:>10} {percentage:>9.1f}% {info:<40}")

    print()
    print("=" * 100)
    print("æŒ‰æ•°æ®æºç»Ÿè®¡é”™è¯¯ç±»å‹")
    print("=" * 100)
    print()

    print(f"{'æ•°æ®æº':<20} {'æ€»é”™è¯¯':>10} {'æ ¼å¼é”™':>10} {'æ£€ç´¢åˆ°æ— ç­”æ¡ˆ':>14} {'æ£€ç´¢é”™æ— ç­”æ¡ˆ':>14} {'æ£€ç´¢é”™':>10} {'æ¨ç†é”™':>10}")
    print("-" * 120)

    for source in sorted(source_error_types.keys()):
        stats = source_error_types[source]
        total_errors = sum(stats.values())
        query_err = stats.get('query_format_error', 0)
        no_ans_correct = stats.get('no_answer_with_correct_docs', 0)
        no_ans_retrieval = stats.get('no_answer_retrieval_error', 0)
        retrieval_err = stats.get('retrieval_error', 0)
        reasoning_err = stats.get('reasoning_error', 0)

        print(f"{source:<20} {total_errors:>10} {query_err:>10} {no_ans_correct:>14} {no_ans_retrieval:>14} {retrieval_err:>10} {reasoning_err:>10}")

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    if output_path is None:
        output_path = json_path.replace('.json', '_error_analysis.txt')

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("=" * 100 + "\n")
        f.write("BadCase é”™è¯¯ç±»å‹è¯¦ç»†åˆ†æ\n")
        f.write("=" * 100 + "\n\n")

        f.write(f"æ€»æ ·æœ¬æ•°: {len(results)}\n")
        f.write(f"é”™è¯¯æ ·æœ¬æ•°: {len(badcases)}\n")
        f.write(f"å‡†ç¡®ç‡: {(len(results) - len(badcases)) / len(results) * 100:.2f}%\n\n")

        # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„å±•ç¤º
        for error_type in ["query_format_error", "no_answer_with_correct_docs", "no_answer_retrieval_error", "retrieval_error", "reasoning_error"]:
            cases = error_types[error_type]
            if not cases:
                continue

            f.write("\n" + "=" * 100 + "\n")
            f.write(f"{error_type.upper()}: {type_info[error_type]}\n")
            f.write(f"å…± {len(cases)} ä¸ªæ ·æœ¬ ({len(cases)/total*100:.1f}%)\n")
            f.write("=" * 100 + "\n\n")

            # ç‰¹åˆ«æ ‡æ³¨é‡ç‚¹å…³æ³¨çš„ç±»å‹
            if error_type == "query_format_error":
                f.write("ğŸ”´ ä¸¥é‡é—®é¢˜ï¼šæ¨¡å‹ç”Ÿæˆäº†é”™è¯¯æ ¼å¼çš„æŸ¥è¯¢ï¼Œå¯¼è‡´æ£€ç´¢å¤±è´¥ï¼\n")
                f.write("    å¸¸è§é”™è¯¯ï¼š\n")
                f.write("    1. <search query=\"...\"> </search> (XML å±æ€§æ ¼å¼ï¼Œåº”è¯¥æ˜¯ <search>...</search>)\n")
                f.write("    2. <search></search> (ç©ºæŸ¥è¯¢)\n")
                f.write("    3. <search>... (æ²¡æœ‰é—­åˆæ ‡ç­¾)\n")
                f.write("    4. æŸ¥è¯¢è¿‡é•¿ï¼ˆè¶…è¿‡ 200 å­—ç¬¦ï¼‰\n")
                f.write("    è§£å†³æ–¹æ¡ˆï¼š\n")
                f.write("    - æ£€æŸ¥ Prompt æ˜¯å¦ç»™å‡ºäº†æ­£ç¡®çš„æ ¼å¼ç¤ºä¾‹\n")
                f.write("    - è®­ç»ƒæ•°æ®ä¸­æ˜¯å¦æœ‰æ ¼å¼é”™è¯¯çš„æ ·æœ¬\n")
                f.write("    - è€ƒè™‘å¢åŠ æ ¼å¼çº¦æŸçš„ reward\n\n")

            elif error_type == "no_answer_with_correct_docs":
                f.write("âš ï¸  é‡ç‚¹å…³æ³¨ï¼šè¿™äº›æ ·æœ¬æ£€ç´¢åˆ°äº†æ­£ç¡®ç­”æ¡ˆï¼Œä½†æ¨¡å‹æ²¡æœ‰ç”Ÿæˆ <answer> æ ‡ç­¾ï¼\n")
                f.write("    å¯èƒ½åŸå› ï¼š\n")
                f.write("    1. è¾¾åˆ° max_turns é™åˆ¶ï¼Œè¢«å¼ºåˆ¶åœæ­¢\n")
                f.write("    2. æ¨¡å‹åˆ¤æ–­ä¿¡æ¯ä¸è¶³ï¼Œæƒ³ç»§ç»­æœç´¢ä½†ä¸èƒ½äº†\n")
                f.write("    3. æ¨¡å‹é™·å…¥å¾ªç¯æœç´¢ï¼Œå¿˜è®°ç»™ç­”æ¡ˆ\n")
                f.write("    4. Prompt æˆ–è®­ç»ƒé—®é¢˜ï¼Œæ¨¡å‹æ²¡å­¦ä¼šä½•æ—¶ç»™ç­”æ¡ˆ\n\n")

            # æ¯ç§ç±»å‹å±•ç¤ºå‰ N ä¸ªæ ·æœ¬
            for i, case in enumerate(cases[:sample_size], 1):
                f.write(f"\n{'â”€' * 100}\n")
                f.write(f"ç¤ºä¾‹ {i}/{min(sample_size, len(cases))}\n")
                f.write(f"{'â”€' * 100}\n\n")

                f.write(f"é—®é¢˜: {case.question}\n")
                f.write(f"æ•°æ®æº: {case.data_source}\n")
                f.write(f"æ ‡å‡†ç­”æ¡ˆ: {case.golden_answer}\n")
                f.write(f"æ¨¡å‹ç­”æ¡ˆ: {case.extracted_answer}\n")
                f.write(f"æœç´¢æ¬¡æ•°: {case.num_searches}\n")
                f.write(f"é”™è¯¯è¯´æ˜: {case.explanation}\n")

                f.write(f"\nå®Œæ•´è½¨è¿¹:\n")
                f.write("â”€" * 100 + "\n")
                f.write(case.full_trajectory[:2000])  # é™åˆ¶é•¿åº¦
                if len(case.full_trajectory) > 2000:
                    f.write("\n... (å·²æˆªæ–­)")
                f.write("\n" + "â”€" * 100 + "\n")

    print()
    print("=" * 100)
    print(f"âœ“ è¯¦ç»†åˆ†æå·²ä¿å­˜åˆ°: {output_path}")
    print(f"  æ¯ç§é”™è¯¯ç±»å‹å±•ç¤ºå‰ {sample_size} ä¸ªæ ·æœ¬")
    print("=" * 100)

    # è¿”å›ç»Ÿè®¡ä¿¡æ¯
    return {
        'total': total,
        'query_format_error': len(error_types['query_format_error']),
        'no_answer_with_correct_docs': len(error_types['no_answer_with_correct_docs']),
        'no_answer_retrieval_error': len(error_types['no_answer_retrieval_error']),
        'retrieval_error': len(error_types['retrieval_error']),
        'reasoning_error': len(error_types['reasoning_error']),
        'by_source': dict(source_error_types)
    }


def main():
    parser = argparse.ArgumentParser(description="åˆ†æ BadCase é”™è¯¯ç±»å‹")
    parser.add_argument('json_path', type=str, help='è¯„ä¼°ç»“æœ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', type=str, default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--sample_size', '-n', type=int, default=10,
                       help='æ¯ç§é”™è¯¯ç±»å‹å±•ç¤ºçš„æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ 10ï¼‰')

    args = parser.parse_args()

    analyze_badcases(args.json_path, args.output, args.sample_size)


if __name__ == '__main__':
    main()
