# TriviaQA Training on Linux with 4x A800 GPUs (GPU 4,5,6,7)

å®Œæ•´çš„ Linux æœåŠ¡å™¨è®­ç»ƒæŒ‡å—ï¼Œä½¿ç”¨ 4 å¼  A800 GPU (ç¼–å· 4,5,6,7)ã€‚

---

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Linux (Ubuntu 18.04+, CentOS 7+)
- **GPU**: 4x A800 80GB (GPU 4,5,6,7)
- **CUDA**: 11.8 æˆ–æ›´é«˜
- **Python**: 3.9 æˆ– 3.10
- **ç£ç›˜ç©ºé—´**: è‡³å°‘ 200GB

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨ï¼ˆ5 æ­¥ï¼‰

### ç¬¬ 1 æ­¥ï¼šç¯å¢ƒå®‰è£…

```bash
# åˆ›å»ºä¸»è®­ç»ƒç¯å¢ƒ
conda create -n searchr1 python=3.9 -y
conda activate searchr1
pip install torch==2.4.0
pip install vllm==0.6.3
cd /path/to/Search-R1  # ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„
pip install -e .

# åˆ›å»ºæ£€ç´¢å™¨ç¯å¢ƒ
conda create -n retriever python=3.10 -y
conda activate retriever
pip install transformers datasets pyserini uvicorn fastapi
```

**æ£€æŸ¥ CUDA ç‰ˆæœ¬**ï¼š
```bash
nvidia-smi
nvcc --version
```

---

### ç¬¬ 2 æ­¥ï¼šä¸‹è½½ BM25 ç´¢å¼•

```bash
# è®¾ç½®ä¿å­˜è·¯å¾„ï¼ˆä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„ï¼‰
export SAVE_PATH=/path/to/wiki_data

# ä¸‹è½½ BM25 ç´¢å¼•å’Œè¯­æ–™åº“
huggingface-cli download PeterJinGo/wiki-18-bm25-index \
    --repo-type dataset \
    --local-dir $SAVE_PATH
```

**å¦‚æœä¸‹è½½é€Ÿåº¦æ…¢ï¼Œä½¿ç”¨å›½å†…é•œåƒ**ï¼š
```bash
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download PeterJinGo/wiki-18-bm25-index \
    --repo-type dataset \
    --local-dir $SAVE_PATH
```

ä¸‹è½½å®Œæˆåæ£€æŸ¥æ–‡ä»¶ï¼š
```bash
ls -lh $SAVE_PATH
# åº”è¯¥çœ‹åˆ°:
# - bm25/ (ç›®å½•)
# - wiki-18.jsonl (æ–‡ä»¶)
```

---

### ç¬¬ 3 æ­¥ï¼šå¤„ç† TriviaQA æ•°æ®

```bash
conda activate searchr1
cd /path/to/Search-R1

# è¿è¡Œæ•°æ®å¤„ç†è„šæœ¬
bash process_triviaqa_data.sh
```

**é¢„æœŸè¾“å‡º**ï¼š
```
============================================
Processing TriviaQA Dataset
============================================
Output directory: ./data/triviaqa_search
============================================
Loading dataset...
Processing train split...
Processing test split...
============================================
Data processing completed!
Files saved to: ./data/triviaqa_search
- train.parquet
- test.parquet
============================================
```

**éªŒè¯æ•°æ®**ï¼š
```bash
ls -lh ./data/triviaqa_search/
# åº”è¯¥çœ‹åˆ°:
# - train.parquet
# - test.parquet
```

---

### ç¬¬ 4 æ­¥ï¼šå¯åŠ¨ BM25 æ£€ç´¢æœåŠ¡

**ä¿®æ”¹é…ç½®**ï¼š
```bash
# ç¼–è¾‘ retrieval_launch_bm25.sh
nano retrieval_launch_bm25.sh

# ä¿®æ”¹ç¬¬ 4 è¡Œä¸ºä½ çš„å®é™…è·¯å¾„:
file_path=/your/actual/path/to/wiki_data
```

**å¯åŠ¨æœåŠ¡ï¼ˆåœ¨ tmux æˆ– screen ä¼šè¯ä¸­ï¼‰**ï¼š
```bash
# æ–¹æ³• 1: ä½¿ç”¨ tmux
tmux new -s bm25
conda activate retriever
bash retrieval_launch_bm25.sh
# æŒ‰ Ctrl+B ç„¶å D åˆ†ç¦»ä¼šè¯

# æ–¹æ³• 2: ä½¿ç”¨ screen
screen -S bm25
conda activate retriever
bash retrieval_launch_bm25.sh
# æŒ‰ Ctrl+A ç„¶å D åˆ†ç¦»ä¼šè¯

# æ–¹æ³• 3: ä½¿ç”¨ nohup
conda activate retriever
nohup bash retrieval_launch_bm25.sh > bm25.log 2>&1 &
```

**éªŒè¯æœåŠ¡è¿è¡Œ**ï¼š
```bash
# æ£€æŸ¥ç«¯å£
netstat -tuln | grep 8000
# æˆ–
ss -tuln | grep 8000

# æµ‹è¯• API
curl -X POST http://127.0.0.1:8000/retrieve \
    -H "Content-Type: application/json" \
    -d '{"query": "test", "topk": 3}'
```

---

### ç¬¬ 5 æ­¥ï¼šå¯åŠ¨ 4 å¡è®­ç»ƒ

**ä¿®æ”¹è®­ç»ƒè„šæœ¬é…ç½®**ï¼š
```bash
# ç¼–è¾‘ train_triviaqa_qwen25_3b_4gpu.sh
nano train_triviaqa_qwen25_3b_4gpu.sh

# ä¿®æ”¹ç¬¬ 11 è¡Œçš„æ•°æ®è·¯å¾„:
DATA_DIR=/your/actual/path/to/Search-R1/data/triviaqa_search
```

**ç»™è„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™**ï¼š
```bash
chmod +x train_triviaqa_qwen25_3b_4gpu.sh
chmod +x retrieval_launch_bm25.sh
chmod +x process_triviaqa_data.sh
```

**å¯åŠ¨è®­ç»ƒ**ï¼š
```bash
# ç¡®ä¿ BM25 æœåŠ¡æ­£åœ¨è¿è¡Œ
# å¯åŠ¨è®­ç»ƒï¼ˆæ¨èåœ¨ tmux ä¸­è¿è¡Œï¼‰
tmux new -s training
conda activate searchr1
bash train_triviaqa_qwen25_3b_4gpu.sh

# åˆ†ç¦» tmux: Ctrl+B ç„¶å D
# é‡æ–°è¿æ¥: tmux attach -t training
```

**ä½¿ç”¨ nohup åå°è®­ç»ƒ**ï¼š
```bash
conda activate searchr1
nohup bash train_triviaqa_qwen25_3b_4gpu.sh > training.log 2>&1 &

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
tail -f training.log
```

---

## âš™ï¸ 4 å¡è®­ç»ƒé…ç½®è¯¦è§£

### GPU åˆ†é…

```bash
export CUDA_VISIBLE_DEVICES=4,5,6,7  # ä½¿ç”¨ GPU 4,5,6,7
```

**éªŒè¯ GPU å¯è§æ€§**ï¼š
```bash
CUDA_VISIBLE_DEVICES=4,5,6,7 python -c "import torch; print(torch.cuda.device_count())"
# åº”è¯¥è¾“å‡º: 4
```

### å…³é”®è®­ç»ƒå‚æ•°ï¼ˆ4 å¡ A800 ä¼˜åŒ–ï¼‰

| å‚æ•° | å€¼ | è¯´æ˜ |
|-----|-----|------|
| `TRAIN_BATCH_SIZE` | 1024 | æ€» batch sizeï¼ˆ4 å¡å…±äº«ï¼‰|
| `PPO_MINI_BATCH_SIZE` | 512 | PPO mini batch |
| `PPO_MICRO_BATCH_SIZE` | 64 | æ¯å¡ micro batch |
| `GPU_MEMORY_UTIL` | 0.7 | A800 80GB å¯è®¾é«˜ä¸€äº› |
| `N_GPUS_PER_NODE` | 4 | ä½¿ç”¨ 4 å¼  GPU |
| `ACTOR_LR` | 1e-6 | Actor å­¦ä¹ ç‡ |
| `CRITIC_LR` | 1e-5 | Critic å­¦ä¹ ç‡ |

### æ˜¾å­˜ä½¿ç”¨ä¼°ç®—

- **æ¯å¡æ˜¾å­˜å ç”¨**: çº¦ 18-22GB
- **æ€»æ˜¾å­˜å ç”¨**: çº¦ 72-88GB
- **A800 80GB**: å®Œå…¨å¤Ÿç”¨ï¼Œè¿˜æœ‰ä½™é‡

### å¦‚æœæ˜¾å­˜ä¸è¶³

ç¼–è¾‘ `train_triviaqa_qwen25_3b_4gpu.sh`ï¼Œè°ƒæ•´è¿™äº›å‚æ•°ï¼š

```bash
# é™ä½ batch size
TRAIN_BATCH_SIZE=512
PPO_MINI_BATCH_SIZE=256
PPO_MICRO_BATCH_SIZE=32

# é™ä½ GPU å†…å­˜ä½¿ç”¨ç‡
GPU_MEMORY_UTIL=0.5
```

---

## ğŸ“Š ç›‘æ§è®­ç»ƒè¿›åº¦

### æ–¹æ³• 1: å®æ—¶æŸ¥çœ‹æ—¥å¿—

```bash
# å¦‚æœä½¿ç”¨ tmux
tmux attach -t training

# å¦‚æœä½¿ç”¨ nohup
tail -f training.log

# åªçœ‹å…³é”®æŒ‡æ ‡
tail -f training.log | grep -E "Epoch|Reward|Loss"
```

### æ–¹æ³• 2: ç›‘æ§ GPU ä½¿ç”¨

```bash
# å®æ—¶ç›‘æ§æ‰€æœ‰ GPU
watch -n 1 nvidia-smi

# åªç›‘æ§ GPU 4,5,6,7
watch -n 1 'nvidia-smi -i 4,5,6,7'

# ä½¿ç”¨ gpustatï¼ˆæ›´ç¾è§‚ï¼‰
pip install gpustat
watch -n 1 gpustat -i 4,5,6,7
```

### æ–¹æ³• 3: WandB å¯è§†åŒ–

```bash
# ç™»å½• WandB
conda activate searchr1
wandb login

# è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° WandB
# è®¿é—® https://wandb.ai æŸ¥çœ‹å®æ—¶æ›²çº¿
```

### æ–¹æ³• 4: TensorBoardï¼ˆå¯é€‰ï¼‰

```bash
# å¦‚æœæ—¥å¿—åŒ…å« TensorBoard æ ¼å¼
tensorboard --logdir ./checkpoints/triviaqa-search-r1-ppo-qwen2.5-3b-instruct-bm25-em-4gpu \
    --bind_all \
    --port 6006

# åœ¨æµè§ˆå™¨è®¿é—®: http://your-server-ip:6006
```

---

## ğŸ”§ å¸¸è§é—®é¢˜æ’æŸ¥

### é—®é¢˜ 1: "CUDA_VISIBLE_DEVICES ä¸ç”Ÿæ•ˆ"

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ˜¾å¼è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=4,5,6,7

# éªŒè¯
python -c "import os; print(os.environ.get('CUDA_VISIBLE_DEVICES'))"

# åœ¨è®­ç»ƒè„šæœ¬ä¸­å†æ¬¡ç¡®è®¤
echo $CUDA_VISIBLE_DEVICES
```

### é—®é¢˜ 2: "æ— æ³•è¿æ¥åˆ° BM25 æœåŠ¡"

**æ’æŸ¥æ­¥éª¤**ï¼š
```bash
# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
ps aux | grep retrieval_server

# æ£€æŸ¥ç«¯å£æ˜¯å¦ç›‘å¬
netstat -tuln | grep 8000

# æµ‹è¯•è¿æ¥
curl http://127.0.0.1:8000/retrieve

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
tail -f bm25.log  # å¦‚æœä½¿ç”¨ nohup
# æˆ–
tmux attach -t bm25  # å¦‚æœä½¿ç”¨ tmux
```

### é—®é¢˜ 3: "Java not found (pyserini éœ€è¦)"

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ–¹æ³• 1: ä½¿ç”¨ conda å®‰è£…
conda activate retriever
conda install openjdk=11 -y

# æ–¹æ³• 2: ç³»ç»Ÿå®‰è£…
# Ubuntu/Debian
sudo apt update
sudo apt install openjdk-11-jdk -y

# CentOS/RHEL
sudo yum install java-11-openjdk -y

# éªŒè¯
java -version
```

### é—®é¢˜ 4: "å¤šå¡è®­ç»ƒä¸å‡è¡¡"

**æ£€æŸ¥è´Ÿè½½**ï¼š
```bash
watch -n 1 'nvidia-smi -i 4,5,6,7 --query-gpu=index,memory.used,utilization.gpu --format=csv'
```

**å¯èƒ½åŸå› **ï¼š
- æ•°æ®å¹¶è¡Œé…ç½®ä¸æ­£ç¡®
- Batch size è®¾ç½®è¿‡å°
- vLLM é…ç½®é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# ç¡®ä¿ tensor_model_parallel_size=1ï¼ˆæ•°æ®å¹¶è¡Œæ¨¡å¼ï¼‰
actor_rollout_ref.rollout.tensor_model_parallel_size=1

# å¢å¤§ batch size
TRAIN_BATCH_SIZE=1024  # ç¡®ä¿èƒ½è¢« 4 æ•´é™¤
```

### é—®é¢˜ 5: "OOM (Out of Memory)"

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# ç¼–è¾‘è®­ç»ƒè„šæœ¬ï¼Œé™ä½è¿™äº›å‚æ•°
TRAIN_BATCH_SIZE=512
PPO_MINI_BATCH_SIZE=256
PPO_MICRO_BATCH_SIZE=32
GPU_MEMORY_UTIL=0.5

# æˆ–è€…ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
actor_rollout_ref.actor.gradient_accumulation_steps=2
```

### é—®é¢˜ 6: "æƒé™é—®é¢˜"

```bash
# ç»™è„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™
chmod +x *.sh

# æ£€æŸ¥æ•°æ®ç›®å½•æƒé™
ls -ld ./data/triviaqa_search
chmod -R 755 ./data
```

---

## ğŸ“ˆ è®­ç»ƒé¢„æœŸç»“æœ

### è®­ç»ƒè¾“å‡ºç¤ºä¾‹

```
============================================
TriviaQA Training with Qwen2.5-3B-Instruct
4x A800 GPUs (GPU 4,5,6,7)
============================================
Data directory: /path/to/data/triviaqa_search
Base model: Qwen/Qwen2.5-3B-Instruct
Experiment name: triviaqa-search-r1-ppo-qwen2.5-3b-instruct-bm25-em-4gpu
GPUs: 4,5,6,7
Number of GPUs: 4
============================================

Epoch 1/10 | Step 10/500
- Average Reward: 0.32
- KL Divergence: 0.0015
- Actor Loss: 0.28
- Critic Loss: 0.22
- Learning Rate: 8.5e-7
- GPU Memory (4/5/6/7): 19GB/19GB/20GB/19GB

Epoch 1/10 | Step 50/500
- Average Reward: 0.45
- KL Divergence: 0.0012
- Actor Loss: 0.21
- Critic Loss: 0.18
...
```

### GPU ä½¿ç”¨æƒ…å†µ

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   4  NVIDIA A800 80GB... On  | 00000000:34:00.0 Off |                    0 |
| N/A   45C    P0   250W / 300W |  19500MiB / 81920MiB |     95%      Default |
|   5  NVIDIA A800 80GB... On  | 00000000:35:00.0 Off |                    0 |
| N/A   46C    P0   252W / 300W |  19600MiB / 81920MiB |     96%      Default |
|   6  NVIDIA A800 80GB... On  | 00000000:36:00.0 Off |                    0 |
| N/A   44C    P0   248W / 300W |  19400MiB / 81920MiB |     94%      Default |
|   7  NVIDIA A800 80GB... On  | 00000000:37:00.0 Off |                    0 |
| N/A   45C    P0   251W / 300W |  19550MiB / 81920MiB |     95%      Default |
+-------------------------------+----------------------+----------------------+
```

### è®­ç»ƒæ—¶é—´ä¼°ç®—

- **å•ä¸ª epoch**: çº¦ 20-30 åˆ†é’Ÿï¼ˆ4å¡ A800ï¼‰
- **æ€»è®­ç»ƒæ—¶é—´**: çº¦ 3-5 å°æ—¶ï¼ˆ10 epochsï¼‰
- **æ¯”å•å¡å¿«**: çº¦ 3.5-4 å€

### æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡

åœ¨ TriviaQA æµ‹è¯•é›†ä¸Šçš„é¢„æœŸç»“æœï¼š
- **EM Score**: 45-55%
- **å¹³å‡æœç´¢è½®æ¬¡**: 2-3
- **æœç´¢æˆåŠŸç‡**: 75-85%

---

## ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

å¦‚æœè®­ç»ƒä¸­æ–­ï¼Œå¯ä»¥ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼š

```bash
# ç¼–è¾‘è®­ç»ƒè„šæœ¬ï¼Œæ·»åŠ æ¢å¤å‚æ•°
nano train_triviaqa_qwen25_3b_4gpu.sh

# åœ¨ python å‘½ä»¤ä¸­æ·»åŠ :
    +trainer.load_checkpoint=./checkpoints/triviaqa-search-r1-ppo-qwen2.5-3b-instruct-bm25-em-4gpu/actor/step_300 \

# é‡æ–°å¯åŠ¨è®­ç»ƒ
bash train_triviaqa_qwen25_3b_4gpu.sh
```

---

## ğŸ“¦ è®­ç»ƒå®Œæˆå

### 1. æ£€æŸ¥ç‚¹ä½ç½®

```bash
ls -lh ./checkpoints/triviaqa-search-r1-ppo-qwen2.5-3b-instruct-bm25-em-4gpu/

# åº”è¯¥çœ‹åˆ°:
# - actor/        (Actor æ¨¡å‹æ£€æŸ¥ç‚¹)
# - critic/       (Critic æ¨¡å‹æ£€æŸ¥ç‚¹)
# - step_100/
# - step_200/
# - ...
```

### 2. è¯„ä¼°æ¨¡å‹

åˆ›å»ºè¯„ä¼°è„šæœ¬ `eval_checkpoint.sh`:

```bash
#!/bin/bash
CHECKPOINT_DIR=./checkpoints/triviaqa-search-r1-ppo-qwen2.5-3b-instruct-bm25-em-4gpu/actor
DATA_DIR=./data/triviaqa_search

export CUDA_VISIBLE_DEVICES=4  # è¯„ä¼°åªéœ€è¦ 1 å¼ å¡

python -m verl.trainer.main_ppo \
    data.val_files=$DATA_DIR/test.parquet \
    data.val_batch_size=128 \
    actor_rollout_ref.model.path=$CHECKPOINT_DIR \
    +trainer.val_only=true \
    +trainer.val_before_train=true \
    max_turns=4 \
    retriever.url="http://127.0.0.1:8000/retrieve" \
    retriever.topk=3
```

è¿è¡Œè¯„ä¼°ï¼š
```bash
chmod +x eval_checkpoint.sh
bash eval_checkpoint.sh
```

### 3. å¯¼å‡ºæ¨¡å‹

```bash
# å¯¼å‡ºä¸º HuggingFace æ ¼å¼
python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint_path = './checkpoints/triviaqa-search-r1-ppo-qwen2.5-3b-instruct-bm25-em-4gpu/actor'
output_path = './models/triviaqa-qwen2.5-3b-final'

model = AutoModelForCausalLM.from_pretrained(checkpoint_path)
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-3B-Instruct')

model.save_pretrained(output_path)
tokenizer.save_pretrained(output_path)
print(f'Model exported to {output_path}')
"
```

---

## ğŸ¯ é«˜çº§é…ç½®

### ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆQwen2.5-7Bï¼‰

```bash
# ç¼–è¾‘è®­ç»ƒè„šæœ¬
nano train_triviaqa_qwen25_3b_4gpu.sh

# ä¿®æ”¹æ¨¡å‹è·¯å¾„
BASE_MODEL=Qwen/Qwen2.5-7B-Instruct

# å¯èƒ½éœ€è¦è°ƒæ•´æ˜¾å­˜é…ç½®
GPU_MEMORY_UTIL=0.6
PPO_MICRO_BATCH_SIZE=32
```

### ä½¿ç”¨ GRPO è€Œé PPO

```bash
# ä½¿ç”¨ GRPO è®­ç»ƒè„šæœ¬
cp train_triviaqa_qwen25_3b_4gpu.sh train_triviaqa_qwen25_3b_4gpu_grpo.sh

# ç¼–è¾‘è„šæœ¬ï¼Œä¿®æ”¹ç®—æ³•é…ç½®
algorithm.adv_estimator=grpo \
actor_rollout_ref.actor.use_kl_loss=true \
actor_rollout_ref.actor.kl_loss_coef=0.001 \
actor_rollout_ref.rollout.n_agent=5 \
```

### å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

```bash
# æ·»åŠ åˆ°è®­ç»ƒè„šæœ¬
actor_rollout_ref.actor.use_fp16=true \
critic.use_fp16=true \
```

---

## ğŸ“ å®Œæ•´å‘½ä»¤é€ŸæŸ¥è¡¨

| æ“ä½œ | å‘½ä»¤ |
|------|------|
| æ¿€æ´»è®­ç»ƒç¯å¢ƒ | `conda activate searchr1` |
| æ¿€æ´»æ£€ç´¢ç¯å¢ƒ | `conda activate retriever` |
| ä¸‹è½½ BM25 ç´¢å¼• | `huggingface-cli download PeterJinGo/wiki-18-bm25-index` |
| å¤„ç†æ•°æ® | `bash process_triviaqa_data.sh` |
| å¯åŠ¨ BM25 æœåŠ¡ | `tmux new -s bm25; bash retrieval_launch_bm25.sh` |
| å¯åŠ¨è®­ç»ƒ | `tmux new -s training; bash train_triviaqa_qwen25_3b_4gpu.sh` |
| ç›‘æ§ GPU | `watch -n 1 nvidia-smi -i 4,5,6,7` |
| æŸ¥çœ‹æ—¥å¿— | `tail -f training.log` |
| è¯„ä¼°æ¨¡å‹ | `bash eval_checkpoint.sh` |

---

## ğŸ†˜ è·å–å¸®åŠ©

- ğŸ“– å®Œæ•´æŒ‡å—: `TriviaQA_BM25_Qwen_Guide.md`
- ğŸ› é—®é¢˜åé¦ˆ: https://github.com/PeterGriffinJin/Search-R1/issues
- ğŸ’¬ ç¤¾åŒºè®¨è®º: https://github.com/PeterGriffinJin/Search-R1/discussions

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸš€

å¦‚æœ‰é—®é¢˜ï¼Œè¯·åŠæ—¶æ£€æŸ¥æ—¥å¿—å¹¶å‚è€ƒæ•…éšœæ’æŸ¥éƒ¨åˆ†ã€‚